<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Justin Dixon on Justin Dixon</title>
    <link>/</link>
    <description>Recent content in Justin Dixon on Justin Dixon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +1100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Contest Theory</title>
      <link>/post/2018-02-28-contest-theory/</link>
      <pubDate>Wed, 28 Feb 2018 21:13:14 -0500</pubDate>
      
      <guid>/post/2018-02-28-contest-theory/</guid>
      <description>&lt;div id=&#34;what-is-contest-theory&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is Contest Theory?&lt;/h1&gt;
&lt;p&gt;Contest theory is a tool to describe situation where agents compete with costly efforts to win a scare prize. Quick tips: - agent: economic agent abstraction, examples include workers, sports people etc. - costly efforts: This just means the agent has to make a &lt;em&gt;decision&lt;/em&gt;. Ie they do have infinity effort to try to win. Think of the contest people tennis players. A player cannot give 110%, it is physically impossible. - scare prize: This forces the agents to compete as they both cannot win in the contest.&lt;/p&gt;
&lt;p&gt;Contest theory deals with various ways contests can be formulated and tried to make predictions on how rational agents would act.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;p&gt;As with many game theorical frameworks many situations can be appropriated. These examples are heavily influenced and borrowed from &lt;span class=&#34;citation&#34;&gt;Konrad (2012)&lt;/span&gt; and I would recommend reading the paper. - &lt;em&gt;R&amp;amp;D Race&lt;/em&gt;: When firms compete against each to win the prize of a patent we can say they are in a contest. Think of pharmaceuticals attempting to find some new cure for a disease. The first firm to discover and patent the discovery wins the prize of the government protected monopoly to sell the drug whilst the other company gets nothing for all their hard work. - &lt;em&gt;Corporate Promotion Ladder&lt;/em&gt;: Executives in large companies are often attempting to gain a promotion. There are obviously limited higher positions available and the executives must decided how hard they wish to work to attempt to win the promotion. - &lt;em&gt;Sports Contest&lt;/em&gt;: Sport contests are perphaps the easiest acticities to conceptually reconcile with contest theory. Think of a tennis match in the final of a grand slam. The winner gains the higher prizepool and the fame. The cost of effort include the physical pain of the match and the possibility of trying to hard leading to injury which would affect future possible earnings.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-maths&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The maths&lt;/h1&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

\pi_{1} = p_{1}(e_{1}, p_{2}) * v - c(e_{1})

\end{equation}\]&lt;/span&gt;
&lt;p&gt;Here we see that the profit or utility of player 1 (in a 2 player contest) is equal to the probability of them winning (which is dictated by the contest success function) times the prize minus the cost of the players effort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;A must read is the novel &lt;span class=&#34;citation&#34;&gt;Konrad and others (2009)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-konrad2012dynamic&#34;&gt;
&lt;p&gt;Konrad, Kai A. 2012. “Dynamic Contests and the Discouragement Effect.” &lt;em&gt;Revue d’économie Politique&lt;/em&gt; 122 (2). Dalloz: 233–56.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-konrad2009strategy&#34;&gt;
&lt;p&gt;Konrad, Kai A, and others. 2009. “Strategy and Dynamics in Contests.” &lt;em&gt;OUP Catalogue&lt;/em&gt;. Oxford University Press.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Discouragement Effect</title>
      <link>/post/2018-03-06-discouragement-effect/</link>
      <pubDate>Wed, 28 Feb 2018 21:13:14 -0500</pubDate>
      
      <guid>/post/2018-03-06-discouragement-effect/</guid>
      <description>&lt;div id=&#34;what-is-the-discouragement-effect&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What is the Discouragement Effect?&lt;/h1&gt;
&lt;p&gt;The discouragement effect is when the future consequences of winning or losing the current contest leads to decreased effort &lt;span class=&#34;citation&#34;&gt;Konrad (2012)&lt;/span&gt;. This is due the future contest reducing the overall value of winning. Think about a tennis match with 3 sets. If a player loses the first set then to win the entire match they must win 2 sets compared to the other player only needing to win 1 match. Therefore they have a harder path to victory than the other player and have an incentives to reduce their effort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example&lt;/h1&gt;
&lt;p&gt;Now let us work through an example. We will continue with our tennis exposition and place the contest as the final at the Australian Open. To win a player must be the first player to win 3 sets.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

playerPayoff = probabilityOfWinning * (prizeOfWinning - prizeOfLosing) - costOfTrying

\end{equation}\]&lt;/span&gt;
&lt;p&gt;Remember that the probabilityOfWinning is the output of function called the Contest Success Function with takes both players efforts as inputs and the costOfTrying is the function with the input of the player’s effort. To make this less dry as possible lets say the grand final is between Roger and Nadal and that Roger is winnign 2 sets to 1. This means that if Roger wins the next set we wins the championship but id Nadal wins then both players have 2 sets each and the final set winner decides the total winner.&lt;/p&gt;
&lt;p&gt;Now if Roger wins he gains the netPrize, which we will give the value of 1. If Nadal wins then both players are tied at 2 sets each. To work out the valuation of being in this state we need to introduce the Tullock model for the CSF.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

porbabilityOfWinning_{Roger} = \frac{effort_{Roger}}{effort_{Roger} + effort_{Nadal}} 

\end{equation}\]&lt;/span&gt;
&lt;p&gt;Pretty neat huh. Now to simplify let us make the costOfTrying to be unity ie&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}


costOfTrying = effort

\end{equation}\]&lt;/span&gt;
&lt;p&gt;Now we have the tools to work out the payoffs for both Roger and Nadal when they are tied at 2 sets each. If either player wins they gain the prize, which we have said is 1.&lt;/p&gt;
&lt;p&gt;Now we need to solve the optimisation problem.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

\frac{\partial payoffRoger}{\partial effortRoger} = \frac{\partial payoffNadal}{\partial effortNadal}  

\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

\frac{effortNadal}{(effortNadal + effortRoger)^2} * 1 - 1 = \frac{effortroger}{(effortNadal + effortRoger)^2} * 1 - 1  

\end{equation}\]&lt;/span&gt;
&lt;p&gt;Simplifying we achieve &lt;span class=&#34;math inline&#34;&gt;\(effortNadal = effortRoger\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now if we put this back in our original equation&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

payoff = \frac{effort}{2*effort} * 1 - effort

\end{equation}\]&lt;/span&gt;
&lt;p&gt;Simplify to&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

payoff = \frac{1}{2} * 1 - effort

\end{equation}\]&lt;/span&gt;
&lt;p&gt;Therefore effort and payoff are equal to 0.25.&lt;/p&gt;
&lt;p&gt;Now this means that Roger faces winning and gaining 1 or losing and gaining 0.25 and Nadal facing winning to gain 0.25 and losing to gain 0. Therefore the value for winning to Roger is 0.75 and for Nadal it is 0.25. This causes asymmetries in winning valuations.&lt;/p&gt;
&lt;p&gt;Now lets worked out how hard the players play at 2 sets to 1.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

payoffRoger = \frac{effortRoger}{effortRoger + effortNadal} * 0.75 - effortRoger

\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

payoffNadal = \frac{effortNadal}{effortRoger + effortNadal} * 0.25 - effortNadal

\end{equation}\]&lt;/span&gt;
&lt;p&gt;Now we need to work out:&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

\frac{\partial payoffRoger}{\partial effortRoger} = \frac{\partial payoffNadal}{\partial effortNadal}  

\end{equation}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

\frac{effortNadal}{(effortNadal + effortRoger)^2} * 0.75 - 1 = \frac{effortRoger}{(effortNadal + effortRoger)^2} * 0.25 - 1  

\end{equation}\]&lt;/span&gt;
&lt;p&gt;so &lt;span class=&#34;math inline&#34;&gt;\(effortNadal * 0.75 = effortRoger * 0.25\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Which simplifies to &lt;span class=&#34;math inline&#34;&gt;\(3*effortNadal = effortRoger\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now subbing these back into our equations we get&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

\frac{effortNadal}{(effortNadal + 3*effortNadal)^2} * 0.75 = 1

\end{equation}\]&lt;/span&gt;
&lt;p&gt;This simplifies to give an effortNadal of 0.046875.&lt;/p&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}

\frac{effortRoger}{(0.333*effortRoger + effortRoger)^2} * 0.25 = 1

\end{equation}\]&lt;/span&gt;
&lt;p&gt;This gives an effortRoger of 0.140625 which also satifies out equality given before.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;\(0.046875 + 0.140625 = 0.1875 &amp;lt; 0.5\)&lt;/span&gt; Therefore our total effort has been reduced in this contest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;Give this one a go &lt;span class=&#34;citation&#34;&gt;Konrad (2012)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-konrad2012dynamic&#34;&gt;
&lt;p&gt;Konrad, Kai A. 2012. “Dynamic Contests and the Discouragement Effect.” &lt;em&gt;Revue d’économie Politique&lt;/em&gt; 122 (2). Dalloz: 233–56.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Multiarmed Bandits: Chapter 1 IRL</title>
      <link>/post/2018-03-10-multiarmed-bandits/</link>
      <pubDate>Sat, 10 Feb 2018 21:13:14 -0500</pubDate>
      
      <guid>/post/2018-03-10-multiarmed-bandits/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This is going to be part of series where I illustrate examples and questions from the brilliant book by Sutton and Barto &lt;span class=&#34;citation&#34;&gt;Sutton and Barto (1998)&lt;/span&gt;. You can download the pdf version of the newly updated book online, just google it. I am planning on going through each chapter and illustrating 1 or 2 examples from each chapter.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiarmed-bandits&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;MultiArmed Bandits&lt;/h1&gt;
&lt;p&gt;What on earth is a mutliarmed bandit? It might be easier to think of it as which pokie you choose to play on down at the local. Say there are 10 pokies at the pub, then you need to decide which machine to game at. But you don’t know which machine will pay at the most so you need to explore and discover where the odds are at.&lt;/p&gt;
&lt;p&gt;Lets as describe a metric; return per dollar spent. That is for every dollar you put in a machine how much do get as a return? (0, 1.5,-.5)&lt;/p&gt;
&lt;p&gt;Let us setup 10 pokie machines with normal distributions but with differing mean returns.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rewardMeans &amp;lt;- rnorm(10)
plot(rewardMeans)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-10-Multiarmed-Bandits_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From our little plot we can see that the best pokie is machine 3.&lt;/p&gt;
&lt;p&gt;Now let us set up our learning problem for our little agent to learn the best machine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-algorithm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The algorithm&lt;/h1&gt;
&lt;span class=&#34;math display&#34;&gt;\[\begin{equation}
\text{for the number of actions k, create:} \\
\\
valueFunction(action) = 0 \\
countFunction(action) = 0 \\
\\

Repeat until iteration count is reached: \\\
action &amp;lt;-
\begin{cases}
 \text{The action that maxes the valueFuntion with prob (1 - epsilon)} \\ 
 \text{A random action with prob epsilon} \\
\end{cases}\\
\\
reward &amp;lt;- bandit(action) \\
countFunction(action) += 1 \\
valueFunction(action) += \frac{1}{countFunction(action)}*(reward - valueFunction(action)) \\
\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;programming-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Programming Example&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;averagerewardAverage &amp;lt;- vector()
averageactions &amp;lt;- vector()


for(w in 1:500) {
# Setup our functions
# Number of pokies
k &amp;lt;- 10
valueFunction &amp;lt;- vector()
countFunction &amp;lt;- vector()

for(i in 1:k) {
  valueFunction[i] &amp;lt;- 0
  countFunction[i] &amp;lt;- 0
}

# Number of steps
T &amp;lt;- 1000
rewardTotal &amp;lt;- vector()
rewardAverage &amp;lt;- vector()
actions &amp;lt;- vector()
for(x in 1:T) {
  epsilon &amp;lt;- 0.1
  if(epsilon &amp;gt; sample(c(0:9), size=1)) {
    action &amp;lt;- sample(c(1:k), size=1)
  } else {
     action &amp;lt;- which(valueFunction==max(valueFunction))[1]
  }
  actions[x] &amp;lt;- action
  # reward &amp;lt;- bandit(action)
  reward &amp;lt;- rnorm(1, mean=rewardMeans[action])
  rewardTotal[x] &amp;lt;- reward
  rewardAverage[x] &amp;lt;- mean(rewardTotal)
  countFunction[action] &amp;lt;- countFunction[action] +  1
  valueFunction[action] &amp;lt;- valueFunction[action] + (1 / countFunction[action])*(reward - valueFunction[action])
}

averagerewardAverage &amp;lt;- rbind(averagerewardAverage,rewardAverage)
averageactions &amp;lt;- rbind(averageactions, actions)
}

averagerewardAverage &amp;lt;- colMeans(averagerewardAverage)
averageactions &amp;lt;- colMeans(averageactions)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;let-us-plot-our-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let us plot our results&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(averagerewardAverage)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-10-Multiarmed-Bandits_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now considering our best slot machine has a payout of around 1.5 it seems out model is not that great at finding the best machine. This highlights the huge problem with reinforcement learning. That is it is HARD.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;optimistic-initial-values-with-greedy-epsilon-0&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Optimistic Initial values with greedy epsilon = 0&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;averagerewardAverage &amp;lt;- vector()
averageactions &amp;lt;- vector()


for(w in 1:500) {
# Setup our functions
# Number of pokies
k &amp;lt;- 10
valueFunction &amp;lt;- vector()
countFunction &amp;lt;- vector()

for(i in 1:k) {
  valueFunction[i] &amp;lt;- 10
  countFunction[i] &amp;lt;- 10
}

# Number of steps
T &amp;lt;- 1000
rewardTotal &amp;lt;- vector()
rewardAverage &amp;lt;- vector()
actions &amp;lt;- vector()
for(x in 1:T) {
  epsilon &amp;lt;- 0
  if(epsilon &amp;gt; sample(c(0:9), size=1)) {
    action &amp;lt;- sample(c(1:k), size=1)
  } else {
     action &amp;lt;- which(valueFunction==max(valueFunction))[1]
  }
  actions[x] &amp;lt;- action
  # reward &amp;lt;- bandit(action)
  reward &amp;lt;- rnorm(1, mean=rewardMeans[action])
  rewardTotal[x] &amp;lt;- reward
  rewardAverage[x] &amp;lt;- mean(rewardTotal)
  countFunction[action] &amp;lt;- countFunction[action] +  1
  valueFunction[action] &amp;lt;- valueFunction[action] + (1 / countFunction[action])*(reward - valueFunction[action])
}

averagerewardAverage &amp;lt;- rbind(averagerewardAverage,rewardAverage)
averageactions &amp;lt;- rbind(averageactions, actions)
}

averagerewardAverage &amp;lt;- colMeans(averagerewardAverage)
averageactions &amp;lt;- colMeans(averageactions)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;let-us-plot-our-results-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let us plot our results&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(averagerewardAverage)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-10-Multiarmed-Bandits_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;constant-step-size&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Constant Step Size&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;averagerewardAverage &amp;lt;- vector()
averageactions &amp;lt;- vector()


for(w in 1:500) {
# Setup our functions
# Number of pokies
k &amp;lt;- 10
valueFunction &amp;lt;- vector()
countFunction &amp;lt;- vector()

for(i in 1:k) {
  valueFunction[i] &amp;lt;- 0
  countFunction[i] &amp;lt;- 0
}

# Number of steps
T &amp;lt;- 1000
rewardTotal &amp;lt;- vector()
rewardAverage &amp;lt;- vector()
actions &amp;lt;- vector()
for(x in 1:T) {
  epsilon &amp;lt;- 1
  if(epsilon &amp;gt; sample(c(0:9), size=1)) {
    action &amp;lt;- sample(c(1:k), size=1)
  } else {
     action &amp;lt;- which(valueFunction==max(valueFunction))[1]
  }
  actions[x] &amp;lt;- action
  # reward &amp;lt;- bandit(action)
  reward &amp;lt;- rnorm(1, mean=rewardMeans[action])
  rewardTotal[x] &amp;lt;- reward
  rewardAverage[x] &amp;lt;- mean(rewardTotal)
  countFunction[action] &amp;lt;- countFunction[action] +  1
  valueFunction[action] &amp;lt;- valueFunction[action] + 0.05*(reward - valueFunction[action])
}

averagerewardAverage &amp;lt;- rbind(averagerewardAverage,rewardAverage)
averageactions &amp;lt;- rbind(averageactions, actions)
}

averagerewardAverage &amp;lt;- colMeans(averagerewardAverage)
averageactions &amp;lt;- colMeans(averageactions)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;let-us-plot-our-results-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Let us plot our results&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot(averagerewardAverage)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-10-Multiarmed-Bandits_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Pretty cool. Next post will be on Markov Decision Processes.&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-sutton1998reinforcement&#34;&gt;
&lt;p&gt;Sutton, Richard S, and Andrew G Barto. 1998. &lt;em&gt;Reinforcement Learning: An Introduction&lt;/em&gt;. Vol. 1. 1. MIT press Cambridge.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>XOR Deep Learning Example</title>
      <link>/post/2018-02-02-xor-deep-learning/</link>
      <pubDate>Fri, 02 Feb 2018 21:13:14 -0500</pubDate>
      
      <guid>/post/2018-02-02-xor-deep-learning/</guid>
      <description>&lt;div id=&#34;the-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Problem&lt;/h1&gt;
&lt;p&gt;OpenAI recently released some open research questions. As a beginner in AI I decided to tackle the begineer ‘Warmups’ they have offered. You can view their blog post &lt;a href=&#34;https://blog.openai.com/requests-for-research-2/&#34;&gt;here&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;⭐ Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequence’s end. Test the two approaches below&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generate a dataset of random 100,000 binary strings of length 50. Train the LSTM; what performance do you get?&lt;/li&gt;
&lt;li&gt;Generate a dataset of random 100,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;the-xor-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The XOR Function&lt;/h1&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Input&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Input 2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Output&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-1-100000-binary-strings-of-length-50&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Problem 1: 100,000 Binary Strings Of Length 50&lt;/h1&gt;
&lt;p&gt;The code for solving this problem is thanks to &lt;a href=&#34;https://github.com/christopher5106/grid-1D-LSTM-theano/blob/master/main.py&#34;&gt;christopher5106&lt;/a&gt;&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
&amp;#39;&amp;#39;&amp;#39;
Trains a 1D Grid LSTM network to learn XOR answers.
&amp;#39;&amp;#39;&amp;#39;

from __future__ import print_function
import numpy as np
import theano
import theano.tensor as T
import lasagne

#Lasagne Seed for Reproducibility
lasagne.random.set_rng(np.random.RandomState(1))

import argparse
parser = argparse.ArgumentParser()
parser.add_argument(&amp;#39;--iterations&amp;#39;, type=int, default=100000, help=&amp;#39;Number of iterations&amp;#39;)
parser.add_argument(&amp;#39;--bits&amp;#39;, type=int, default=50, help=&amp;#39;Number of bits in the input strings&amp;#39;)
parser.add_argument(&amp;#39;--hidden&amp;#39;, type=int, default=2, help=&amp;#39;Number of units in the two hidden (LSTM) layers&amp;#39;)
parser.add_argument(&amp;#39;--learning_rate&amp;#39;, type=float, default=0.5, help=&amp;#39;Optimization learning rate&amp;#39;)
#parser.add_argument(&amp;#39;--grad_clip&amp;#39;, type=int, default=100, help=&amp;#39;All gradients above this will be clipped&amp;#39;)
parser.add_argument(&amp;#39;--print_freq&amp;#39;, type=int, default=500, help=&amp;#39;How often should we check the output?&amp;#39;)
parser.add_argument(&amp;#39;--batch_size&amp;#39;, type=int, default=1, help=&amp;#39;Batch size&amp;#39;)
parser.add_argument(&amp;#39;--layers&amp;#39;, type=int, default=2, help=&amp;#39;Number of layers&amp;#39;)
args = parser.parse_args()

print(&amp;quot;Parameters:&amp;quot;)
print(args)
args.print_batch_freq = args.print_freq / args.batch_size + 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;random-string-generator&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random String Generator&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
def gen_data(bits=args.bits, batch_size = args.batch_size):
    x = np.random.randint(2, size=(batch_size,bits))
    y = x.sum(axis=1) % 2
    return x, y&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;network&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Network&lt;/h1&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;
print(&amp;quot;Building network ...&amp;quot;)

l_in = lasagne.layers.InputLayer(shape=(None,1))
l_in_zero = lasagne.layers.InputLayer(shape=(None, args.layers, 1))
l_lin = lasagne.layers.DenseLayer(l_in, num_units = args.hidden, nonlinearity = None)

l_forward = lasagne.layers.LSTMLayer(
   l_in_zero, args.hidden,
    nonlinearity=lasagne.nonlinearities.tanh, hid_init = l_lin, only_return_final=True)

l_lin_out = lasagne.layers.DenseLayer(l_forward, num_units = 2, nonlinearity = None)
l_out = lasagne.layers.DenseLayer(l_lin_out, num_units=2, nonlinearity=lasagne.nonlinearities.softmax)

target_values = T.ivector(&amp;#39;target_output&amp;#39;)

network_output = lasagne.layers.get_output(l_out)

cost = T.nnet.categorical_crossentropy(network_output,target_values).mean()
accuracy = lasagne.objectives.categorical_accuracy(network_output,target_values).mean()

all_params = lasagne.layers.get_all_params(l_out,trainable=True)
all_params2 = lasagne.layers.get_all_params(l_out,trainable=False)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;theano-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Theano Functions&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Computing updates ...&amp;quot;)
updates = lasagne.updates.adadelta(cost, all_params, args.learning_rate)
updates2 = lasagne.updates.adagrad(cost, all_params2, args.learning_rate)

print(&amp;quot;Compiling functions ...&amp;quot;)
train = theano.function([l_in.input_var, l_in_zero.input_var, target_values], cost, updates=updates, allow_input_downcast=True)
train2 = theano.function([l_in.input_var, l_in_zero.input_var, target_values], cost, updates=updates2, allow_input_downcast=True)
compute_cost = theano.function([l_in.input_var, l_in_zero.input_var, target_values], cost, allow_input_downcast=True)
compute_accuracy = theano.function([l_in.input_var, l_in_zero.input_var, target_values],
    accuracy, allow_input_downcast=True)

probs = theano.function([l_in.input_var, l_in_zero.input_var],network_output,allow_input_downcast=True)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;training&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;print(&amp;quot;Training ...&amp;quot;)
print(&amp;quot;The average loss and accuracy will be printed every {} iterations&amp;quot;.format(args.print_batch_freq*args.batch_size))
num_batch_print_iter = args.iterations / args.batch_size / args.print_batch_freq + 1
act_num_batches =  int(num_batch_print_iter * args.print_batch_freq)
all_cost = np.zeros((act_num_batches))
all_accuracy = np.zeros((act_num_batches))

for it_out in range(int(num_batch_print_iter)):
    for it_in in range(int(args.print_batch_freq)):
        x,y = gen_data()
        stoplen = (len(x[0])-1)
        for i in range(0,stoplen):
            x_zero = np.zeros((args.batch_size,args.layers,1),dtype=&amp;#39;int32&amp;#39;)
            batch_cost = train2(np.reshape(x[0][i], newshape=(1,1)), x_zero, y)
        x_zero = np.zeros((args.batch_size, args.layers, 1), dtype=&amp;#39;int32&amp;#39;)
        batch_iter = int(it_out * args.print_batch_freq + it_in + 1)
        batch_cost = train(np.reshape(x[0][len(x[0])-1], newshape=(1,1)), x_zero, y)
        batch_accuracy = compute_accuracy(np.reshape(x[0][len(x[0])-1], newshape=(1,1)), x_zero, y)

        all_cost[batch_iter - 1] = batch_cost
        all_accuracy[batch_iter - 1] = batch_accuracy
    start_index = int(it_out * args.print_batch_freq)
    end_index = int((it_out + 1) * args.print_batch_freq)
    av_cost = all_cost[start_index:end_index].mean()
    av_accuracy = all_accuracy[start_index:end_index].mean()
    np.savetxt(&amp;#39;cost.txt&amp;#39;, all_cost[:end_index], delimiter=&amp;#39;,&amp;#39;)  #average in batch
    np.savetxt(&amp;#39;accuracy.txt&amp;#39;, all_accuracy[:end_index], delimiter=&amp;#39;,&amp;#39;)

    print(&amp;quot;Iteration {} average loss = {} average accuracy = {}&amp;quot;.format(batch_iter*args.batch_size,
av_cost,av_accuracy))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;problem-2-100000-binary-strings-of-random-length-150&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Problem 2: 100,000 Binary Strings Of Random Length 1:50&lt;/h1&gt;
&lt;div id=&#34;code-differences&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Code differences&lt;/h2&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;parser.add_argument(&amp;#39;--hidden&amp;#39;, type=int, default=100, help=&amp;#39;Number of units in the two hidden (LSTM) layers&amp;#39;)
parser.add_argument(&amp;#39;--layers&amp;#39;, type=int, default=2, help=&amp;#39;Number of layers&amp;#39;)
args = parser.parse_args()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;def gen_data(bits=args.bits, batch_size = args.batch_size):
    bitsran = np.random.randint(1, bits)
    x = np.random.randint(2, size=(batch_size,bitsran))
    y = x.sum(axis=1) % 2
    return x, y&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Decision Trees</title>
      <link>/post/2018-01-22-decisioin-trees/</link>
      <pubDate>Wed, 31 Jan 2018 21:13:14 -0500</pubDate>
      
      <guid>/post/2018-01-22-decisioin-trees/</guid>
      <description>&lt;p&gt;Have you been struggling to learn about what decision trees are? Finding it difficult to link pictures of trees with machine learning algorithms? If you answered yes to these questions then this post is for you.&lt;/p&gt;
&lt;p&gt;Decision trees are an amazingly powerful predictive machine learning method that all Data Analysts should know. When I was researching tree-based methods I could never find a hand worked problem. Most other souces simply list the maths, or show the results of a grown tree. To truly understand the method though I needed to see how trees are actually grown! So I worked it out and now to save time for you I have put my working into this post.&lt;/p&gt;
&lt;p&gt;The majority of the theory involved in this post is thanks to this paper &lt;span class=&#34;citation&#34;&gt;(Breiman et al. 1984)&lt;/span&gt; whilst the mathematics is taken from &lt;span class=&#34;citation&#34;&gt;(Friedman, Hastie, and Tibshirani 2001)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The structure of this post follows closely to how I learn, and how I hope you learn! First I will list examples of how decision trees have been used and their advantages and disadvantages. Next Ill present a worked example and following will be an example of classification and regression example.&lt;/p&gt;
&lt;div id=&#34;examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;p&gt;Decision Trees are used in a wide variety of fields! These examples are in credit to &lt;a href=&#34;https://github.com/michaeldorner/DecisionTrees&#34;&gt;Micheal Dorner&lt;/a&gt;. I have used decision trees in my Masters thesis too.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Astronomy:&lt;/strong&gt; Distinguishing between stars and cosmic rays in images collected by the Hubble Space Telescope.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Medicine:&lt;/strong&gt; Diagnosis of the ovarian cancer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Economy:&lt;/strong&gt; Stock trading.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Geography:&lt;/strong&gt; To predict and correct errors in topographical and geological data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Personally:&lt;/strong&gt; Predict the probabilities of winning for teams in professional Dota 2 matches.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;table-of-advantages-and-disadvantages&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Table of advantages and disadvantages&lt;/h1&gt;
&lt;center&gt;
&lt;table style=&#34;width:78%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;37%&#34; /&gt;
&lt;col width=&#34;40%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Advantages&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Disadvantages&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Easy to understand&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Overfits the training data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Resistant to outliers and weak features&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Stuggles with continuous depedent variables&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Easy to implement in practice&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Need important variables&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Can handle datasets with missing values and errors&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Trees are unstable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Makes no assumptions about the underlying distributions&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Lack of smoothness&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/center&gt;
&lt;p&gt;Advantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Easy to understand:&lt;/strong&gt; When a decision tree is constructed you can view the decision rules in a nice looking tree diagram, hence the name!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Resistant to outliers and weak features:&lt;/strong&gt; The splitting criteria does not care greatly how far values are from the decision boundary. The splitting criteria splits according the strongest features first which minimises the harm of the weak features (the weak features are splitting already split data = smaller effect).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Easy to implement in practice:&lt;/strong&gt; As the model is resistant to outliers and weak features in practice you do not need to spend as much time testing different feature input combinations.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Can handle datasets with missing values and errors:&lt;/strong&gt; Similar to being resistant to outliers and weak features.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Makes no assumptions about the underlying distributions:&lt;/strong&gt; This may not so important in practice but it makes the model theorectically more appealing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadvantages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Overfits the training data&lt;/strong&gt;: This is a very large issue but can be minimised using random forests. Which we will cover in the next post.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Stuggles with continuous depedent variables:&lt;/strong&gt; Due to the leafs containing several observations which are averaged the prediction space is not smooth. This makes highly accurate regression predictions difficult to achieve.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Need important variables:&lt;/strong&gt; Without strong predictors tree based methods lose many of their strengths.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Trees are unstable:&lt;/strong&gt; The structure of an estimated tree can vary significantly between different estimations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-algorithm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Algorithm&lt;/h1&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Start at the root node.&lt;/li&gt;
&lt;li&gt;For each input, find the set &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; that minimizes the sum of the node impurities in the two child nodes and choose the split &lt;span class=&#34;math inline&#34;&gt;\(\{X \in S \}\)&lt;/span&gt; that gives the minimum overall &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;If a stopping criterion is reached, exit. Otherwise, apply step 2 to each child node in turn.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In simplier words to build the tree you need to decide where the splits(branches) are going to happen. You need to calculate for each possible split for each input variable an ‘impurity’ measure. For classification trees this impurity measure can be the Gini Index. This is just a measure that says how well the split divides the data. The smallest impurity score is where the split will happen.&lt;/p&gt;
&lt;p&gt;After you have divided the data into two regions you continue to split those regions again and again until you reach some stopping rule. Once the stopping rule is reached it is possible to &lt;em&gt;prune&lt;/em&gt; the tree. This is typically done occuring to some &lt;em&gt;cost complexity&lt;/em&gt; measure where non-terminal splits can be removed.&lt;/p&gt;
&lt;div id=&#34;splitting-criteria---gini-impurity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting criteria - Gini Impurity&lt;/h2&gt;
&lt;p&gt;There are several splitting criteria that can be used but I will be using the Gini method for this worked example. We can define the Gini Impuirty as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ GiniImpurity = \sum_{k \neq k&amp;#39;} \hat{p}_{mk}\hat{p}_{mk&amp;#39;} = \sum^{K}_{k=1} \hat{p}_{mk}(1-\hat{p}_{mk}) \]&lt;/span&gt; where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \hat{p}_{mk} = \frac{1}{N_{m}}\sum_{x_{i}\in R_{m}} I(y_{i} = k)  \]&lt;/span&gt; and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ N_{i} = \#\{x_{i} \in R_{m} \} \]&lt;/span&gt; and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ R_{1}(j,s) = \{X|X \leq s \} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[R_{2}(j,s) = \{X|X &amp;gt; s \} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stopping-criteria---minimum-leaf-size&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stopping criteria - Minimum Leaf Size&lt;/h2&gt;
&lt;p&gt;There are several possible rules for when the splitting algorithm should stop. These possibilities include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If a split region only has identical values of the dependent variable then that region will not be split any futher.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;If all cases in a node have identical values for each predictor, the node will not be split.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;If the current tree depth reaches the user-specified maximum tree depth limit value, the tree growing process will stop.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;If the size of a node is less than the user-specified minimum node size value, the node will not be split.&lt;/li&gt;
&lt;li&gt;If the split of a node results in a child node whose node size is less than the user-specified minimum child node size value, the node will not be split.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other stopping criteria could include an error minimisation rule, but this tends to miss important splits that could happen. That is why it is preferable to grow the tree and then prune it back to an acceptable level. For this tutorial a minimum leaf size of 5 was chosen.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prunning-criteria---misclarification-rate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prunning Criteria - Misclarification Rate&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \frac{1}{N_{m}} \sum_{i \in R_{m}} I(y_{i} \neq k(m)) = 1 - \hat{p}_{mk(m)} \]&lt;/span&gt; where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ k(m) = argmax_{k}\hat{p}_{mk} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;worked-example&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Worked Example&lt;/h1&gt;
&lt;p&gt;Let us begin with a worked simple example. It always helps to understand the intuition to see the basics of the method being used.&lt;/p&gt;
&lt;p&gt;Lets us build a simple dataset to work the problem by hand. The dataset will be ten observations of two classes, 0 or 1, with two predictors, X1 and X2.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Class &amp;lt;- as.factor(c(0,0,0,0,0,1,1,1,1,1)) # The 2 class vector
X1 &amp;lt;- c(4,4.5,5,5.5,3,5.6,6,6.5,6.2,5.9) # Random values for predictor 1
X2&amp;lt;- c(9,10,11,10,9,8,7,8,7,8) # Similarly
df &amp;lt;- cbind.data.frame(Class, X1, X2) # Combine the class vector and the two predictors&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = df, aes(x = X1, y=X2)) + # Plot the two predictors and colour the 
  ggtitle(label = &amp;quot;The two Predictors with their assoicated Class values&amp;quot;) +
  geom_point(aes(color=Class), size = 6, alpha = .5) # observations according to which class they belong&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-22-decisioin-trees_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From the graph it is obvious how to split the data but lets us worked it out using the algorithm to see how it works. First we will calculate the Gini Impurity for each possible split in the range of each predictor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Predictor1test &amp;lt;- seq(from = 3, to = 7, by  = 0.1) # The potential splits where we calculate the Gini Impurity
Predictor2test &amp;lt;- seq(from =7, to = 11, by = 0.1) # Similar for Predictor 2
CalculateP &amp;lt;- function(i, index, m, k) { # Function to calculate the proportion of observations in the split
  if(m==&amp;quot;L&amp;quot;) {                          # region (m) which match to class (k) 
    Nm &amp;lt;- length(df$Class[which(df[,index] &amp;lt;= i)]) # The number of observations in the split region Rm
    Count &amp;lt;- df$Class[which(df[,index] &amp;lt;= i)] == k # The number of observations that match the class k
  } else {
    Nm &amp;lt;- length(df$Class[which(df[,index] &amp;gt; i)])
    Count &amp;lt;- df$Class[which(df[,index] &amp;gt; i)] == k
  } 
  P &amp;lt;- length(Count[Count==TRUE]) / Nm # Proportion calculation
  return(c(P,Nm)) # Returns both the porportion and the number of observations
}
CalculateGini &amp;lt;- function(x, index) { # Function to calculate the Gini Impurity
  Gini &amp;lt;- NULL # Create the Gini variables
  for(i in x) {
    pl0 &amp;lt;- CalculateP(i, index, &amp;quot;L&amp;quot;, 0) # Proportion in the left region with class 0
    pl1 &amp;lt;- CalculateP(i, index, &amp;quot;L&amp;quot;, 1)
    GiniL &amp;lt;- pl0[1]*(1-pl0[1]) + pl1[1]*(1-pl1[1]) # The Fini for the left region
    pr0 &amp;lt;- CalculateP(i, index, &amp;quot;R&amp;quot;, 0)
    pr1 &amp;lt;- CalculateP(i, index, &amp;quot;R&amp;quot;, 1)
    GiniR &amp;lt;- pr0[1]*(1-pr0[1]) + pr1[1]*(1-pr1[1])
    Gini &amp;lt;- rbind(Gini, sum(GiniL * pl0[2]/(pl0[2] + pr0[2]),GiniR * pr0[2]/(pl0[2] + pr0[2]), na.rm = TRUE)) # Need to weight both left and right Gini scores when combining both
  }
  return(Gini)
}
Gini &amp;lt;- CalculateGini(Predictor1test, 2)
Predictor1test&amp;lt;- cbind.data.frame(Predictor1test, Gini)
Gini &amp;lt;- CalculateGini(Predictor2test, 3)
Predictor2test&amp;lt;- cbind.data.frame(Predictor2test, Gini)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Predictor1test, aes(x=Predictor1test, y=Gini)) + 
  ggtitle(&amp;quot;Gini Index For Possible Values Of X1&amp;quot;) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-22-decisioin-trees_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the most pure split for predictor1 is at 5.5. All other splits leave some impurity in the resulting spaces.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Predictor2test, aes(x=Predictor2test, y=Gini)) + 
  ggtitle(&amp;quot;Gini Index For Possible Values Of X2&amp;quot;) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-22-decisioin-trees_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here we can see a region where the Gini Impurity is minimised. Any value here would be suitable. Now we can observe a hand calculation of the Gini Impurity for X1 = 5.5.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/img/ginipart1.png&#34; alt=&#34;optional caption text&#34; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&#34;/img/ginipart2.png&#34; alt=&#34;optional caption text&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;With a minimum off 5 observations per leaf we are already at the stopping criteria but let us see the misclassification for each off our potential stopping criteria for the sake of illumination. Also due to the purity of the split we do not need to prune the tree.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CalculatePkm &amp;lt;- function(i, index, m) { # This is different to the other P function is that it calculates the proportion to
  if(m==&amp;quot;L&amp;quot;) {                          # only the majority class
    Nm &amp;lt;- length(df$Class[which(df[,index] &amp;lt;= i)]) 
    Km &amp;lt;- as.integer(names(sort(table(df$Class[which(df[,index] &amp;lt;= i)]), decreasing = TRUE)[1]))
    Count &amp;lt;- df$Class[which(df[,index] &amp;lt;= i)] == Km
  } else {
    Nm &amp;lt;- length(df$Class[which(df[,index] &amp;gt; i)])
    Km &amp;lt;- as.integer(names(sort(table(df$Class[which(df[,index] &amp;gt; i)]), decreasing = TRUE)[1]))
    Count &amp;lt;- df$Class[which(df[,index] &amp;gt; i)] == Km
  } 
  P &amp;lt;- length(Count[Count==TRUE]) / Nm
  return(c(P,Nm))
}
CalculateMissClass &amp;lt;- function(x, index) {
  miserr &amp;lt;- NULL
  for(i in x) {
    pLkm &amp;lt;- CalculatePkm(i, index, &amp;quot;L&amp;quot;)
    missclassL &amp;lt;- (1 - pLkm[1])
    pRkm &amp;lt;- CalculatePkm(i, index, &amp;quot;R&amp;quot;)
    missclassR &amp;lt;- (1 - pRkm[1])
    miserr &amp;lt;- rbind(miserr,  sum(missclassL * pLkm[2]/(pLkm[2] + pRkm[2]),missclassR * pRkm[2]/(pLkm[2] + pRkm[2]), na.rm = TRUE))
  }
  return(miserr)
}
miserr &amp;lt;- CalculateMissClass(Predictor1test[,1], 2)
Predictor1test&amp;lt;- cbind.data.frame(Predictor1test, miserr)
miserr &amp;lt;- CalculateMissClass(Predictor2test[,1], 3)
Predictor2test&amp;lt;- cbind.data.frame(Predictor2test, miserr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Predictor1test, aes(x=Predictor1test, y=miserr)) + 
  ggtitle(&amp;quot;Misclassification Error for X1&amp;quot;) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-22-decisioin-trees_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = Predictor2test, aes(x=Predictor2test, y=miserr)) + 
  ggtitle(&amp;quot;Misclassification Error for X2&amp;quot;) +
  geom_line()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-22-decisioin-trees_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Similar to the case with the Gini Impurity we can see the regions where the measure is minimised. Now we can go through a hand drawn problem and see the calculation in action.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;/img/missclasspart1.png&#34; alt=&#34;optional caption text&#34; /&gt;
&lt;/center&gt;
&lt;center&gt;
&lt;img src=&#34;/img/misclasspart2.png&#34; alt=&#34;optional caption text&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;Now with such a small dataset it does not make sense to prune the tree but let us continue to see an example with real data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-classification-tree&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example: Classification Tree&lt;/h1&gt;
&lt;p&gt;For this example we will use the iris dataset that comes packed with R. There are three species of the iris flower: setosa, versicolor, and virgincia. We will use the classification tree process to separate the feaures of Sepal Length, Sepal Width, Pdeal Length, and Petal Width.&lt;/p&gt;
&lt;div id=&#34;species-of-iris-with-each-predictor-combination&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Species of Iris with each Predictor Combination&lt;/h2&gt;
&lt;center&gt;
&lt;img src=&#34;/img/IrisInitial.gif&#34; alt=&#34;Iris data plotted with each combination of predictor&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;From the data you can see that Setosa can be separated easily from the other two species. Try it yourself, where would you draw the line to separate Setosa?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;build-the-tree&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Build the tree&lt;/h2&gt;
&lt;p&gt;Now that we have worked through how the classification tree is grown we can resort to using already established packages to estimate our decision tree for us. I will be using the rpart package for this post but there are many other packages that can estimate trees.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tree &amp;lt;- rpart(Species ~ ., data=iris, parms = list(split = &amp;#39;gini&amp;#39;) )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-splits&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Splits&lt;/h2&gt;
&lt;p&gt;The tree growing process can involve many more splits than the single split from our hand worked problem. Here we can see all the splits that were done when growing this tree.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(Tree$splits, digits = 2, format = &amp;#39;markdown&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;count&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ncat&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;improve&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;index&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;adj&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34.16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;150&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.92&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.76&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.83&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.50&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;38.97&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;37.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10.69&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Petal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.91&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.75&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Length&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.15&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sepal.Width&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.67&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.95&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.28&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;There are quite a few splits here! So let us look at the two most important splits, Petal.Length = 2.5 and Petal.Width = 0.8.&lt;/p&gt;
&lt;div id=&#34;where-the-split-happens-with-petallength-2.5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where the split happens with PetalLength = 2.5&lt;/h3&gt;
&lt;center&gt;
&lt;img src=&#34;/img/PetalLength.gif&#34; alt=&#34;Where the split happens with PetalLength = 2.5&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;div id=&#34;where-the-splits-occure-when-petalwidth-0.8&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Where the splits occure when PetalWidth = 0.8&lt;/h3&gt;
&lt;center&gt;
&lt;img src=&#34;/img/PetalWidth.gif&#34; alt=&#34;Where the splits occure when PetalWidth = 0.8&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-final-pruned-tree&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Final Pruned Tree&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(Tree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-22-decisioin-trees_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This is the final tree diagram. You may notice that the tree has been pruned to have only 2 split nodes. Indeed can see that the split for Petal.Width is moved to 1.8 instead of at 0.8 in the final decision tree.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;example-regression-tree&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Example: Regression Tree&lt;/h1&gt;
&lt;p&gt;For this example I will be providing less explanation.&lt;/p&gt;
&lt;div id=&#34;splitting-criteria---minimising-the-sum-of-squares&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Splitting Criteria - Minimising The Sum Of Squares&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \min_{j,s} [\min_{c1} \sum_{x_{i} \in R_{1}(j,s)}(y_{i} -c_{1})^2 +  \min_{c2} \sum_{x_{i} \in R_{2}(j,s)}(y_{i} -c_{2})^2 ] \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \hat{c}_{m} = \frac{1}{N_{m}} \sum_{x_{i} \in R_{m}} y_{i} \]&lt;/span&gt; The equation calculates, for each variable, the sum of squared errors. The value for &lt;span class=&#34;math inline&#34;&gt;\(c_{m}\)&lt;/span&gt; is simply the average observation value in that region. The algoirthm then finds the smallest sum of squares for that variable and does that for each variable. The algorithm finally compares the champion split from each variable to determine the winner for the overall split to occur. The stopping criteria will once again be a minimum size of terminal nodes to be 5.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pruning-criteria---cost-complexity-criteria&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Pruning Criteria - Cost Complexity Criteria&lt;/h2&gt;
&lt;p&gt;Let:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Q_{m}(T) = \frac{1}{N_{m}} \sum_{x_{i} \in R_{m}} (y_{i} - \hat{c}_{m})^2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;we may then define cost complexity criterion as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ C_{\alpha}(T) = \sum^{|T|}_{m=1} N_{m}Q_{m}(T) + \alpha|T| \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Our goal is to minimise this function. The compromise between tree size and its fit to the data is dictated by &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;. Smaller values lead to larger trees and larger values lead to more pruning.&lt;/p&gt;
&lt;p&gt;There are processes by which to estimate &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; but we will not go into that today.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data-usa-arrests&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Data: USA Arrests&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = USArrests, aes(x = Assault, y = UrbanPop)) + 
  geom_point(aes(color=Murder), size = 6, alpha = .5)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-22-decisioin-trees_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RegressionTree &amp;lt;- rpart(Murder~ Assault + UrbanPop, data=USArrests)
kable(RegressionTree$splits, digits = 2, format = &amp;#39;markdown&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;count&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;ncat&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;improve&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;index&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;adj&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Assault&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.66&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;176.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;UrbanPop&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.03&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;57.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;UrbanPop&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.62&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;69.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Assault&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;104.0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;UrbanPop&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;28&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;58.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;UrbanPop&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.79&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;51.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;UrbanPop&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.26&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;66.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Assault&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;22&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;243.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Assault&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.64&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;195.5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div id=&#34;the-first-4-splits&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The First 4 Splits&lt;/h3&gt;
&lt;center&gt;
&lt;img src=&#34;/img/regression.gif&#34; alt=&#34;gif&#34; /&gt;
&lt;/center&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-constructed-tree&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Constructed Tree&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rpart.plot(RegressionTree)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-01-22-decisioin-trees_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;Next up will be a post on Random Forests. How trees are implemented in the real world.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-reading&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Decision_tree_learning&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Decision_tree_learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.stat.cmu.edu/~cshalizi/350-2006/lecture-10.pdf&#34; class=&#34;uri&#34;&gt;http://www.stat.cmu.edu/~cshalizi/350-2006/lecture-10.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://datajobs.com/data-science-repo/Decision-Trees-%5BRokach-and-Maimon%5D.pdf&#34; class=&#34;uri&#34;&gt;https://datajobs.com/data-science-repo/Decision-Trees-[Rokach-and-Maimon].pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf&#34; class=&#34;uri&#34;&gt;http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-breiman1984classification&#34;&gt;
&lt;p&gt;Breiman, Leo, Jerome Friedman, Charles J Stone, and Richard A Olshen. 1984. &lt;em&gt;Classification and Regression Trees&lt;/em&gt;. CRC press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-friedman2001elements&#34;&gt;
&lt;p&gt;Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2001. &lt;em&gt;The Elements of Statistical Learning&lt;/em&gt;. Vol. 1. Springer series in statistics New York.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Random Forests</title>
      <link>/post/2018-03-04-random-forests/</link>
      <pubDate>Wed, 31 Jan 2018 21:13:14 -0500</pubDate>
      
      <guid>/post/2018-03-04-random-forests/</guid>
      <description>&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Following on from the previous post about decision trees let us move on to Random Forests. Let us use the Soybean data from the ‘mlbench’ package. There are 35 features and 683 observations with 16 varieties of Soybean.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-care-about-random-forests&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why care about Random Forests?&lt;/h1&gt;
&lt;p&gt;Let us look at how our decision trees predict previous unseen data. First we will load the data in:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(mlbench)
library(caret)
data(&amp;quot;BreastCancer&amp;quot;)
dim(BreastCancer)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us now split the data up into a training and test data set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BreastCancer &amp;lt;- na.omit(BreastCancer)
BreastCancer$Id &amp;lt;- as.factor(BreastCancer$Id)
BreastCancer &amp;lt;- subset(BreastCancer, select = -Id)
index &amp;lt;- createDataPartition(y = BreastCancer$Class, times = 1, p = 0.75, list=FALSE)
train0 &amp;lt;- BreastCancer[index,]
test0 &amp;lt;- BreastCancer[-index,]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now train a decision tree like we did in the previous tutorial and see how the model did predicting values of the test set.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tree &amp;lt;- caret::train(Class ~., data = train0, method = &amp;#39;rpart&amp;#39;)
#Tree &amp;lt;- rpart(Class ~., data = train0)
results &amp;lt;- caret::predict.train(Tree, newdata = test0)
hello &amp;lt;- caret::confusionMatrix(results, test0$Class)
overall &amp;lt;- hello$overall&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;kable(t(overall), digits = 3, format=&amp;#39;markdown&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;15%&#34; /&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Accuracy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Kappa&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AccuracyLower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AccuracyUpper&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AccuracyNull&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AccuracyPValue&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;McnemarPValue&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.941&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.871&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.894&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.971&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.653&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.752&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(knitr::kable(hello$table))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;benign&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;malignant&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;benign&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;105&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;malignant&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;55&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;That is a pretty good model but lets see if we can improve it. Decision trees are know to have lower bias and this equates to a higher variance. That is decision trees &lt;em&gt;overfit&lt;/em&gt; the training data. Random forests allow us to overcome this issue.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;RF &amp;lt;- caret::train(Class ~ ., data=train0, method = &amp;#39;rf&amp;#39;, ntree= 300)
results &amp;lt;- caret::predict.train(RF, newdata = test0)
hello1 &amp;lt;- caret::confusionMatrix(results, test0$Class)
overall1 &amp;lt;- hello$overall
print(knitr::kable(t(overall1), digits = 3))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;10%&#34; /&gt;
&lt;col width=&#34;7%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;col width=&#34;15%&#34; /&gt;
&lt;col width=&#34;17%&#34; /&gt;
&lt;col width=&#34;16%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;Accuracy&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Kappa&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AccuracyLower&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AccuracyUpper&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AccuracyNull&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;AccuracyPValue&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;McnemarPValue&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.965&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.923&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.925&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.987&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.653&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.683&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(knitr::kable(hello1$table))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;benign&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;malignant&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;benign&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;107&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;malignant&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;57&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;See, we achieved a slightly higher accuracy. That means we successfully classified more cancer patients. That is a pretty good thing, well worth using random forests over a single decision tree.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-algorithm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Algorithm&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;numberOffTrees &amp;lt;- 300
NumberOfTreesInForest &amp;lt;- seq(1, numberOffTrees)
RandomForest &amp;lt;- list()
for(i in NumberOfTreesInForest) {
  # Get a sample of the data size N
  sampleData &amp;lt;- sample(train, size = N)
  
  # Grow a tree
  singleTree &amp;lt;- rpart(Class ~ ., data = train)
  
  # Add the tree to the forest
  RandomForest[i] &amp;lt;- singleTree 
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;regression-prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Regression Prediction&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;predictionData
for(i in NumberOfTreesInForest) {
  # Make a single prediction
  singlePrediction &amp;lt;- rpart.predict(ContinuousY ~ ., data = predictionData)
  
  # Add the tree to the forest
  regressionPrediction =  regressionPrediction + singlePrediction
}
regressionPrediction &amp;lt;- regressionPrediction / numberOffTrees&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;classification-prediction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Classification Prediction&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;predictionData
for(i in NumberOfTreesInForest) {
  # Make a single prediction
  singlePrediction &amp;lt;- rpart.predict(Class ~ ., data = predictionData)
  
  # Add the tree to the forest
  classificationPrediction[i] =  singlePrediction
}
max(table(classificationPrediction))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-benefits&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The benefits&lt;/h1&gt;
&lt;p&gt;The MAJOR benefit of random forests are that they are easy to implement and require little tinkering. Many ML models require tuning parameters whilst Random Forests are about as close to a working model out of the box as you can get.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>/talk/example-talk/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +1100</pubDate>
      
      <guid>/talk/example-talk/</guid>
      <description>&lt;p&gt;Embed your slides or video here using &lt;a href=&#34;https://sourcethemes.com/academic/post/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;. Further details can easily be added using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
